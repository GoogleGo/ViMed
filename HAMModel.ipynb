{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "print(tf.__version__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3, 'mel': 4, 'nv': 5, 'vasc': 6}\n"
     ]
    }
   ],
   "source": [
    "# modelName = input(\"Model Name: \")\n",
    "\n",
    "# Import metadata for age, sex, and localization stats\n",
    "database = pd.read_csv(r\"C:\\Users\\Aiden\\Desktop\\Disease Recognition\\dataverse_files\\HAM10000_metadata.csv\")\n",
    "\n",
    "# Get all the unique values for localization\n",
    "values = list(set(database.localization.values))\n",
    "localizations = {values[i]: i for i in range(len(values))}\n",
    "\n",
    "sex = {\"female\": 1, \"male\": 2, \"unknown\": 0}\n",
    "\n",
    "metaData_x = [[int(database.age.values[i] if not pd.isna(database.age.values[i]) else -1),\n",
    "               sex[database.sex.values[i]],\n",
    "               localizations[database.localization.values[i]],\n",
    "               database.image_id.values[i]\n",
    "\n",
    "               ] for i in range(len(database.age.values))\n",
    "              ]\n",
    "\n",
    "# Numerical ids for output diseases\n",
    "values = list(set(database.dx.values))\n",
    "values.sort()\n",
    "diseases = {values[i]: i for i in range(len(values))}\n",
    "invDiseases = {v: k for k, v in diseases.items()}\n",
    "print(diseases)\n",
    "dataset_y_Sparse = [diseases[database.dx.values[i]] for i in range(len(database.dx.values))]\n",
    "\n",
    "# One hot encode the output\n",
    "dataset_y = tf.keras.utils.to_categorical(dataset_y_Sparse, len(diseases))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Images\n",
      "Loading image 141 of 142 of category vasc. Total Completion: 99.99%%\n",
      "\n",
      "~------ CHECK PASSED ------~\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load all images in the dataset\n",
    "print(\"\\nLoading Images\")\n",
    "dataset_x = []\n",
    "dataset_yS = []\n",
    "metaDat_x = []\n",
    "metaDataCount = 0\n",
    "totalImages = sum([len(files) for r, d, files in\n",
    "                   os.walk(r\"C:\\Users\\Aiden\\Desktop\\Disease Recognition\\dataverse_files\\HAM10000_categories\")])\n",
    "for i in range(7):\n",
    "    for j in os.listdir(r\"C:\\Users\\Aiden\\Desktop\\Disease Recognition\\dataverse_files\\HAM10000_categories\\{}\".format(\n",
    "            invDiseases[i])):\n",
    "        sys.stdout.write(\"\\rLoading image {} of {} of category {}. Total Completion: {}%\".format(\n",
    "            os.listdir(r\"C:\\Users\\Aiden\\Desktop\\Disease Recognition\\dataverse_files\\HAM10000_categories\\{}\".format(\n",
    "                invDiseases[i])).index(j),\n",
    "            len(os.listdir(r\"C:\\Users\\Aiden\\Desktop\\Disease Recognition\\dataverse_files\\HAM10000_categories\\{}\".format(\n",
    "                invDiseases[i]))),\n",
    "            invDiseases[i],\n",
    "            round((metaDataCount / totalImages) * 100, 2))\n",
    "        )\n",
    "        img = Image.open(r\"C:\\Users\\Aiden\\Desktop\\Disease Recognition\\dataverse_files\\HAM10000_categories\\{}\".format(\n",
    "            invDiseases[i]) + \"\\\\\" + j)\n",
    "        img = img.resize((50, 50))\n",
    "\n",
    "        # Normalize image\n",
    "        img = np.array(img)\n",
    "        img = img / 255.0\n",
    "        dataset_x.append(img)\n",
    "        ind = 0\n",
    "        for l in metaData_x:\n",
    "            if l[3] in j:\n",
    "                metaDat_x.append(l[:3])\n",
    "                ind = metaData_x.index(l)\n",
    "                metaDataCount += 1\n",
    "                break\n",
    "        dataset_yS.append(dataset_y[ind])\n",
    "\n",
    "dataset_y = dataset_yS\n",
    "\n",
    "metaData_x = metaDat_x\n",
    "\n",
    "if len(dataset_x) == len(dataset_y) == len(metaData_x) == metaDataCount:\n",
    "    print(\"\\n\\n~------ CHECK PASSED ------~\\n\\n\")\n",
    "else:\n",
    "    print(\"ERROR: Data not equal in length. Terminating due to data corruption.\")\n",
    "    exit(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Randomize Data\n",
    "randomize = np.arange(len(metaData_x))\n",
    "np.random.shuffle(randomize)\n",
    "metaData_x = np.array(metaData_x)[randomize]\n",
    "dataset_x = np.array(dataset_x)[randomize]\n",
    "dataset_y = np.array(dataset_y)[randomize]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "split = int(len(metaData_x) * 0.9)\n",
    "train_x = dataset_x[:split]\n",
    "train_y = dataset_y[:split]\n",
    "train_meta = metaData_x[:split]\n",
    "\n",
    "test_x = dataset_x[split:]\n",
    "test_y = dataset_y[split:]\n",
    "test_meta = metaData_x[split:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Model\n",
    "inputConv = tf.keras.layers.Input(shape=(50, 50, 3), name=\"img\")\n",
    "xConv = tf.keras.layers.Conv2D(256, (3, 3), activation='relu')(inputConv)\n",
    "xConv = tf.keras.layers.Dropout(0.2)(xConv)\n",
    "xConv = tf.keras.layers.MaxPooling2D((2, 2))(xConv)\n",
    "xConv = tf.keras.layers.BatchNormalization()(xConv)\n",
    "xConv = tf.keras.layers.Conv2D(512, (3, 3), activation='relu')(xConv)\n",
    "xConv = tf.keras.layers.Dropout(0.2)(xConv)\n",
    "xConv = tf.keras.layers.MaxPooling2D((2, 2))(xConv)\n",
    "xConv = tf.keras.layers.BatchNormalization()(xConv)\n",
    "xConv = tf.keras.layers.Conv2D(1024, (3, 3), activation='relu')(xConv)\n",
    "xConv = tf.keras.layers.Dropout(0.2)(xConv)\n",
    "xConv = tf.keras.layers.MaxPooling2D((1, 1))(xConv)\n",
    "xConv = tf.keras.layers.Conv2D(1024, (3, 3), activation='relu')(xConv)\n",
    "xConv = tf.keras.layers.Dropout(0.2)(xConv)\n",
    "xConv = tf.keras.layers.MaxPooling2D((1, 1))(xConv)\n",
    "xConv = tf.keras.layers.BatchNormalization()(xConv)\n",
    "xConv = tf.keras.layers.Conv2D(1024, (3, 3), activation='relu')(xConv)\n",
    "xConv = tf.keras.layers.Dropout(0.2)(xConv)\n",
    "xConv = tf.keras.layers.MaxPooling2D((1, 1))(xConv)\n",
    "xConv = tf.keras.layers.BatchNormalization()(xConv)\n",
    "xConv = tf.keras.layers.Flatten()(xConv)\n",
    "xConv = tf.keras.layers.Dense(256, activation='relu')(xConv)\n",
    "xConv = tf.keras.layers.Dropout(0.4)(xConv)\n",
    "\n",
    "inputMeta = tf.keras.layers.Input(shape=(3,), name=\"meta\")\n",
    "xMeta = tf.keras.layers.Dense(32, activation='relu')(inputMeta)\n",
    "xMeta = tf.keras.layers.Dropout(0.2)(xMeta)\n",
    "\n",
    "concat = tf.keras.layers.concatenate([xConv, xMeta])\n",
    "xCombined = tf.keras.layers.Dense(128, activation='leaky_relu')(concat)\n",
    "xCombined = tf.keras.layers.Dropout(0.3)(xCombined)\n",
    "output = tf.keras.layers.Dense(7, activation='softmax')(xCombined)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inputConv, inputMeta], outputs=output)\n",
    "\n",
    "def accuracyK(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=2)\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "282/282 [==============================] - 10s 33ms/step - loss: 2.4983 - accuracy: 0.5917 - val_loss: 1.6238 - val_accuracy: 0.1796 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "282/282 [==============================] - 9s 31ms/step - loss: 1.1272 - accuracy: 0.6347 - val_loss: 0.9788 - val_accuracy: 0.6607 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "282/282 [==============================] - 9s 31ms/step - loss: 1.0491 - accuracy: 0.6509 - val_loss: 0.9814 - val_accuracy: 0.6627 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "282/282 [==============================] - 9s 30ms/step - loss: 0.9276 - accuracy: 0.6697 - val_loss: 0.9940 - val_accuracy: 0.6727 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "282/282 [==============================] - 8s 30ms/step - loss: 0.9045 - accuracy: 0.6779 - val_loss: 1.0041 - val_accuracy: 0.6577 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.8648 - accuracy: 0.6904\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "282/282 [==============================] - 8s 30ms/step - loss: 0.8640 - accuracy: 0.6904 - val_loss: 1.2597 - val_accuracy: 0.6497 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "282/282 [==============================] - 9s 30ms/step - loss: 0.8040 - accuracy: 0.6974 - val_loss: 0.9396 - val_accuracy: 0.6856 - lr: 5.0000e-04\n",
      "Epoch 8/100\n",
      "282/282 [==============================] - 9s 30ms/step - loss: 0.7802 - accuracy: 0.7033 - val_loss: 0.9592 - val_accuracy: 0.6786 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "282/282 [==============================] - 9s 30ms/step - loss: 0.7603 - accuracy: 0.7032 - val_loss: 0.8765 - val_accuracy: 0.6886 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "282/282 [==============================] - 9s 30ms/step - loss: 0.7405 - accuracy: 0.7063 - val_loss: 0.8626 - val_accuracy: 0.6776 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.7348 - accuracy: 0.7081\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "282/282 [==============================] - 8s 30ms/step - loss: 0.7345 - accuracy: 0.7080 - val_loss: 0.8949 - val_accuracy: 0.6826 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "282/282 [==============================] - 8s 30ms/step - loss: 0.7080 - accuracy: 0.7151 - val_loss: 0.8465 - val_accuracy: 0.6796 - lr: 2.5000e-04\n",
      "Epoch 13/100\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.6930 - accuracy: 0.7214\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "282/282 [==============================] - 9s 30ms/step - loss: 0.6935 - accuracy: 0.7212 - val_loss: 0.9599 - val_accuracy: 0.6856 - lr: 2.5000e-04\n",
      "Epoch 14/100\n",
      "281/282 [============================>.] - ETA: 0s - loss: 0.6757 - accuracy: 0.7256Restoring model weights from the end of the best epoch: 9.\n",
      "282/282 [==============================] - 9s 30ms/step - loss: 0.6756 - accuracy: 0.7255 - val_loss: 1.0619 - val_accuracy: 0.6836 - lr: 1.2500e-04\n",
      "Epoch 14: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2bb94ec5310>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', min_delta=0, patience=5, verbose=2,\n",
    "    mode='max', baseline=None, restore_best_weights=True\n",
    ")\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    verbose=1,\n",
    "    cooldown=0,\n",
    "    mode='auto',\n",
    "    min_delta=0.0001,\n",
    "    min_lr=0)\n",
    "\n",
    "# Train Model\n",
    "class_weights = {0: 4.375273044997816, 1: 2.7834908282379103, 2: 1.301832835044846, 3: 12.440993788819876, 4: 1.2854575792581184, 5: 0.21338020666879728, 6: 10.075452716297788}\n",
    "class_weights = {0:1,1:0.5,2:1,3:1,4:1,5:1,6:1}\n",
    "model.fit([train_x, train_meta], train_y,\n",
    "          epochs=100,\n",
    "          class_weight=class_weights,\n",
    "          validation_data=([test_x, test_meta], test_y),\n",
    "          callbacks=[early_stopping, reduce_lr])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 11ms/step - loss: 2.3451 - accuracy: 0.6986\n"
     ]
    },
    {
     "data": {
      "text/plain": "[2.3450992107391357, 0.6986027956008911]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate\n",
    "model.evaluate([test_x[:20], test_meta[:20]], test_y[:20])\n",
    "\n",
    "# Test for overfit\n",
    "model.predict([test_x[:5], test_meta[:5]])\n",
    "print(test_y[:5])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving Model\n"
     ]
    }
   ],
   "source": [
    "# Save Model\n",
    "save = input(\"Save Model? (y/n): \")\n",
    "if save != \"y\":\n",
    "    print(\"\\nModel not saved..\\n\")\n",
    "    exit(0)\n",
    "print(\"\\nSaving Model\")\n",
    "model.save(input(\"Model Name: \") + \".h5\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
